<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>COMET: Commonsense Transformers for Automatic Knowledge Graph Construction - Papers I read</title><meta name="description" content="Some Helpful Pre-readingAtomic Dataset: Reading annotation examples on webpage will suffice. The paper presents a pretty decent data collection strategy.ConceptNet: Understand the entities and relationships. Just explore the web interface. For a deep dive, go to the paper. Pretty mature dataset.BLEU-Score: BLEU (bilingual evaluation understudy) is&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="./comet-commonsense-transformers-for-automatic-knowledge-graph-construction.html"><style>:root{--body-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--heading-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--logo-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--menu-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol"}</style><link rel="stylesheet" href="./assets/css/style.css?v=1d9126d5c592483966037cf187b30969"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"./comet-commonsense-transformers-for-automatic-knowledge-graph-construction.html"},"headline":"COMET: Commonsense Transformers for Automatic Knowledge Graph Construction","datePublished":"2020-11-03T12:44","dateModified":"2020-11-04T15:43","image":{"@type":"ImageObject","url":"./media/posts/3/priscilla-du-preez-CNf31ObmoCs-unsplash.jpg","height":3648,"width":5472},"description":"Some Helpful Pre-readingAtomic Dataset: Reading annotation examples on webpage will suffice. The paper presents a pretty decent data collection strategy.ConceptNet: Understand the entities and relationships. Just explore the web interface. For a deep dive, go to the paper. Pretty mature dataset.BLEU-Score: BLEU (bilingual evaluation understudy) is&hellip;","author":{"@type":"Person","name":"Samagra Sharma"},"publisher":{"@type":"Organization","name":"Samagra Sharma"}}</script><script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};</script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="./">Papers I read</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="./the-three-pass-approach.html" target="_self">The three pass approach</a></li></ul></nav></header><main><article class="post"><div class="hero"><figure class="hero__image hero__image--overlay"><img src="./media/posts/3/priscilla-du-preez-CNf31ObmoCs-unsplash.jpg" srcset="./media/posts/3/responsive/priscilla-du-preez-CNf31ObmoCs-unsplash-xs.jpg 300w, ./media/posts/3/responsive/priscilla-du-preez-CNf31ObmoCs-unsplash-sm.jpg 480w, ./media/posts/3/responsive/priscilla-du-preez-CNf31ObmoCs-unsplash-md.jpg 768w, ./media/posts/3/responsive/priscilla-du-preez-CNf31ObmoCs-unsplash-lg.jpg 1024w, ./media/posts/3/responsive/priscilla-du-preez-CNf31ObmoCs-unsplash-xl.jpg 1360w, ./media/posts/3/responsive/priscilla-du-preez-CNf31ObmoCs-unsplash-2xl.jpg 1600w" sizes="(max-width: 1600px) 100vw, 1600px" loading="eager" height="3648" width="5472" alt=""><figcaption>Knowledge is light. So, open your doors of perception.</figcaption></figure><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2020-11-03T12:44">November 3, 2020</time></div><h1>COMET: Commonsense Transformers for Automatic Knowledge Graph Construction</h1><div class="post__meta post__meta--author"><img src="./media/website/prof_pic.png" loading="eager" alt="Samagra Sharma" class="post__author-thumb"> <a href="./authors/samagra-sharma/" class="feed__author invert">Samagra Sharma</a></div></div></header></div><div class="wrapper post__entry"><h2>Some Helpful Pre-reading</h2><ol><li>Atomic Dataset: Reading annotation examples on webpage will suffice. The paper presents a pretty decent data collection strategy.</li><li><a href="http://www.conceptnet.io/">ConceptNet</a>: Understand the entities and relationships. Just explore the <a href="http://www.conceptnet.io/">web interface</a>. For a deep dive, go to the <a href="https://arxiv.org/abs/1612.03975">paper</a>. Pretty mature dataset.</li><li><a href="https://machinelearningmastery.com/calculate-bleu-score-for-text-python/#:~:text=The%20Bilingual%20Evaluation%20Understudy%20Score,in%20a%20score%20of%200.0.">BLEU-Score</a>: <strong>BLEU</strong> (<strong>bilingual evaluation understudy</strong>) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. BLEU compares the n-gram of the candidate translation with n-gram of the reference translation to count the number of matches. These matches are independent of the positions where they occur. The more the number of matches between candidate and reference translation, the better is the machine translation. <a href="https://en.wikipedia.org/wiki/BLEU">See Algorithm</a>.</li><li><a href="https://en.wikipedia.org/wiki/Beam_search#:~:text=In%20computer%20science%2C%20beam%20search,that%20reduces%20its%20memory%20requirements.">Beam Search</a>: Beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Select the most probable events at each decision step.</li></ol><h2>The first pass</h2><ul><li><strong>Category</strong>:  The paper defines a new downstream task for language models. It uses transformers for open world knowledge graph creation.</li><li><strong>Context</strong>:<ul><li>Knowledge Base Construction is a long standing problem in AI. Some other related areas are:<ul><li>Semi structured text extraction into relational schemas</li><li>Open Information Extraction Approaches do open entities and relations but they are limited to extractive capture</li></ul></li><li>Large scale language model for downstream task (Named Entity Recognition, Part of Speech Tagging etc.</li></ul></li><li><strong>Correctness</strong>:<ul><li>The paper aims at constructing knowledge graphs by fine-tuning pretrained language models. It aims at creating a global intuitive knowledge. I am a bit unsure of the applicability and correctness of such an approach.</li></ul></li><li><strong>Contributions</strong>:<ul><li>Put forth common sense acquisition as knowledge base construction.</li><li>A generative approach to knowledge base construction</li><li>A framework for using large scale transformers for to produce Knowledge bases</li><li>Empirical study of the quality of generated knowledge bases</li></ul></li><li><strong>Clarity</strong>:<br><ul><li>I have a clear picture of how I would approach the problem. I would start by collecting product graphs and product descriptions from e-commerce websites. Transform them into some sort of encoding decoding mechanism, model a generative task and fine tune the transformer. However I am still unclear as to the tuple related details of the paper.</li><li>Another good source of dataset would be encyclopaedic taxonomies (such as Class 9 diversity chapter) or maybe family trees (synthetic descriptions and stuff). So yeah, the directions seems intuitive and clear.</li></ul></li></ul><h2>The Second Pass</h2><ul><li>COMET is a generative model and uses a seed set of knowledge tuples in order to get trained.</li><li><strong>Task:</strong><ul><li>COMET is given a knowledge base of natural language tuples. $\{s,r,o\}$ tuples.<ul><li>$s$: Subject phrase (eg: s="Take a nap")</li><li>$r$: Relation of the tuple (eg: r= "<em>Causes</em>")</li><li>$o$: Object of the tuple (eg: o = "have energy")</li></ul></li><li><strong>The task is to generate $o$ given $s$ and $r$</strong></li></ul></li><li><strong>Transformer Basics: </strong>Let's take a look at the forward pass for the transformer model.<ul><li>Each word is first converted to a D-dimensional embedding similar to all language models.</li><li><figure class="post__image"><img loading="lazy" src="./media/posts/3/word_emb.png" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/word_emb-xs.png 300w, ./media/posts/3/responsive/word_emb-sm.png 480w, ./media/posts/3/responsive/word_emb-md.png 768w, ./media/posts/3/responsive/word_emb-lg.png 1024w, ./media/posts/3/responsive/word_emb-xl.png 1360w, ./media/posts/3/responsive/word_emb-2xl.png 1600w" alt="" width="886" height="279"><figcaption>Source : Felix Hill Deepmind</figcaption></figure><p> </p></li><li> Each embedding is then converted into a set of $[ \mathbf{q},\mathbf{k},\mathbf{v}]$<ul><li>$\mathbf{q}$: query vector</li><li>$\mathbf{k}$: key vector (like hash)</li><li>$\mathbf{v}$: value vector (like content)</li></ul></li><li><figure class="post__image"><img loading="lazy" src="./media/posts/3/transformer_1.png" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/transformer_1-xs.png 300w, ./media/posts/3/responsive/transformer_1-sm.png 480w, ./media/posts/3/responsive/transformer_1-md.png 768w, ./media/posts/3/responsive/transformer_1-lg.png 1024w, ./media/posts/3/responsive/transformer_1-xl.png 1360w, ./media/posts/3/responsive/transformer_1-2xl.png 1600w" alt="" width="733" height="426"><figcaption>Source: Felix Hill (Deepmind)</figcaption></figure></li><li>Self attention($\lambda$) calculation using $\mathbf{q}$ of the current word and $\mathbf{v}$ of the surrounding words.</li><li><figure class="post__image"><img loading="lazy" src="./media/posts/3/t-2.png" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/t-2-xs.png 300w, ./media/posts/3/responsive/t-2-sm.png 480w, ./media/posts/3/responsive/t-2-md.png 768w, ./media/posts/3/responsive/t-2-lg.png 1024w, ./media/posts/3/responsive/t-2-xl.png 1360w, ./media/posts/3/responsive/t-2-2xl.png 1600w" alt="" width="779" height="460"><figcaption>Source: Felix Hill (Deepmind)</figcaption></figure></li><li>Next layer vector calculation using $\lambda$ and $\mathbf{v}$ vectors.</li><li><figure class="post__image"><img loading="lazy" src="./media/posts/3/t-3.png" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/t-3-xs.png 300w, ./media/posts/3/responsive/t-3-sm.png 480w, ./media/posts/3/responsive/t-3-md.png 768w, ./media/posts/3/responsive/t-3-lg.png 1024w, ./media/posts/3/responsive/t-3-xl.png 1360w, ./media/posts/3/responsive/t-3-2xl.png 1600w" alt="" width="792" height="495"><figcaption>Source: Felix Hill(Deepmind)</figcaption></figure></li><li>Since, transformer is temporally invariant, we introduce a slight location bias before and/or after every transformer block.</li><li><figure class="post__image"><img loading="lazy" src="./media/posts/3/t-4.png" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/t-4-xs.png 300w, ./media/posts/3/responsive/t-4-sm.png 480w, ./media/posts/3/responsive/t-4-md.png 768w, ./media/posts/3/responsive/t-4-lg.png 1024w, ./media/posts/3/responsive/t-4-xl.png 1360w, ./media/posts/3/responsive/t-4-2xl.png 1600w" alt="" width="768" height="191"><figcaption>Caption</figcaption></figure></li></ul></li><li>The Datasets: The datasets are essentially graphs represented in (Node, Relation, Node) form. Here are the examples:<ul><li>ConceptNet: eg tuple -&gt; ("Concept Net", "is a", "Semantic Network")<ul><li><figure class="post__image"><img loading="lazy" src="./media/posts/3/conceptnet.png" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/conceptnet-xs.png 300w, ./media/posts/3/responsive/conceptnet-sm.png 480w, ./media/posts/3/responsive/conceptnet-md.png 768w, ./media/posts/3/responsive/conceptnet-lg.png 1024w, ./media/posts/3/responsive/conceptnet-xl.png 1360w, ./media/posts/3/responsive/conceptnet-2xl.png 1600w" alt="" width="920" height="985"><figcaption>Source: Conceptnet Website</figcaption></figure></li></ul></li><li>ATOMIC: eg tuple: ("go to mall", "has prerequisite", "have money")<ul><li><figure class="post__image"><img loading="lazy" src="./media/posts/3/atomic.png" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/atomic-xs.png 300w, ./media/posts/3/responsive/atomic-sm.png 480w, ./media/posts/3/responsive/atomic-md.png 768w, ./media/posts/3/responsive/atomic-lg.png 1024w, ./media/posts/3/responsive/atomic-xl.png 1360w, ./media/posts/3/responsive/atomic-2xl.png 1600w" alt="" width="751" height="776"></figure></li></ul></li></ul></li><li>Conversion to input tokens is done as follows:<ul><li><figure class="post__image"><img loading="lazy" src="./media/posts/3/inp_token_comet.png" sizes="(max-width: 48em) 100vw, 768px" srcset="./media/posts/3/responsive/inp_token_comet-xs.png 300w, ./media/posts/3/responsive/inp_token_comet-sm.png 480w, ./media/posts/3/responsive/inp_token_comet-md.png 768w, ./media/posts/3/responsive/inp_token_comet-lg.png 1024w, ./media/posts/3/responsive/inp_token_comet-xl.png 1360w, ./media/posts/3/responsive/inp_token_comet-2xl.png 1600w" alt="" width="709" height="308"><figcaption>Source: COMET Paper</figcaption></figure></li></ul></li><li>Loss Function: Crossentropy on object tokens $$ L = - \sum_{t=|s|+|r|}^{|s|+|r|+|o|} \log P(x_t | x_{&lt;t})$$</li><li>Metrics (Atomic)<ul><li>BLEU-2 Score: Compares the 2-gram of the candidate translation with 2-gram of the related relations to count the number of matches. I dont understand this metric a lot. Maybe higher leads to more quality.</li><li>$\percentage N/T sro$ : Fraction of generated tuples that are novel. This should be near 100 %.</li><li>$\percentage N/T o$: Fraction of generated objects that are novel. This should also be as high as possible for new knowledge</li><li>$\percentage N/U o$ Fraction of novel objects to unique objects. This should be high as it represents diversity.</li><li>Human Evaluation: Ask AMT workers to identify whether the generated {s,r,o} make sense. <ul><li>100 randomly selected events</li><li>9 attributes for every event</li><li>10 candidates using beam search for every (event,attr) combo</li><li>=&gt; 5000 ratings for each relation</li></ul></li></ul></li><li>Significance testing : Pitman's Test (A non parametric permutation significance test) </li><li> Baselines-Atomic:<ul><li>LSTM Seq2Seq encoder decoder.</li></ul></li><li>Ablation<ul><li>Comet vs Comet (-pretrained)</li><li>Effect of decoding schemes : Greedy vs Beam vs top-k </li></ul></li><li>Results for ATOMIC<ul><li>Beats baseline on BLEU-2 as well as Human Evaluations</li><li>More new objects and tuples than baseline</li><li>Interesting trends wrt amount of training data. Works with low data as well.</li><li>Network initialised with GPT2 weights improves the metric</li><li>COMET could be effective with humans in loop owing to beam search.</li><li>Table 5 shows novel tuples which seem pretty reasonable</li></ul></li><li>Metrics For ConceptNet<ul><li>PPL</li><li>Accuracy of the knowledge being correct by the Bilinear AVG model: Achieves 92.5% accuracy</li><li>Similar novelty metrics as Atomic</li><li>Human Evaluation - similar to ConceptNet</li></ul></li><li>Baselines for ConceptNet<ul><li>BiLSTM</li><li>LSTM -s</li></ul></li><li>Ablations<ul><li>Pretraining</li><li>Relation embedding (Comet RelTok) vs Natural Language Embedding (Main Model)</li></ul></li><li>Results<ul><li>Low PPL</li><li>High Accuracy</li><li>Novel - some times simplified objects</li><li>Edit distance comparisons show that generated tuples are within sufficient distance from the train tuples</li><li>Clear advantages to pretraining (Dove example)</li></ul></li></ul><h2>Extension Ideas</h2><ul><li>Contextualised common sense: At the present the model is trying to learn global/ general truths. It is memorising what usually happens. It would be nice to see models developing common sense in a contextual setting. This is different from when you apply the model on every sentence of the context.</li><li>Knowledge Base Assisted Neural Networks: Can neural networks be designed to take help from these large Knowledge Graphs such as ConceptNet?</li><li>Probabilistic associations with these knowledge graphs.</li><li>Adversarial attack on the model by shuffling the dataset and then seeing if it is merely fitting a function. For every {s,r} select a random o.</li><li>Generating product graphs for ecommerce.</li></ul></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on November 4, 2020</p><ul class="post__tag"><li><a href="./tags/knowledge-discovery/">Knowledge Discovery</a></li><li><a href="./tags/language-grounding/">Natural Language Processing</a></li></ul><div class="post__share"></div><div class="post__bio bio"><img class="bio__avatar" src="./media/website/prof_pic.png" loading="lazy" alt="Samagra Sharma"><div class="bio__info"><h3 class="bio__name"><a href="./authors/samagra-sharma/" class="invert" rel="author">Samagra Sharma</a></h3><p>Samagra is an AI Researcher at Adobe Media and Data Science Research Labs. He graduated from IIT Roorkee with a Bachelor&#x27;s in Computer Science and Engineering.</p></div></div></footer></article><nav class="post__nav"><div class="post__nav-inner"><div class="post__nav-prev"><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="./assets/svg/svg-map.svg#arrow-prev"/></svg> <a href="./the-three-pass-approach.html" class="invert post__nav-link" rel="prev"><span>Previous</span> The Three Pass Approach</a></div></div></nav></main><footer class="footer"><div class="footer__social"><a href="https://www.facebook.com/samagra.sharma/" aria-label="Facebook"><svg><use xlink:href="./assets/svg/svg-map.svg#facebook"/></svg> </a><a href="https://twitter.com/samagra_sharma" aria-label="Twitter"><svg><use xlink:href="./assets/svg/svg-map.svg#twitter"/></svg> </a><a href="https://www.instagram.com/samagra14/" aria-label="Instagram"><svg><use xlink:href="./assets/svg/svg-map.svg#instagram"/></svg> </a><a href="https://www.linkedin.com/in/samagra-sharma-4476bb135/" aria-label="LinkedIn"><svg><use xlink:href="./assets/svg/svg-map.svg#linkedin"/></svg></a></div><div class="footer__copyright"><p>Powered by <a href="https://getpublii.com" target="_blank" rel="nofollow noopener">Publii</a></p></div><button class="footer__bttop js-footer__bttop" aria-label="Back to top"><svg><title>Back to top</title><use xlink:href="./assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'sidebar',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.top',
   };</script><script defer="defer" src="./assets/js/scripts.min.js?v=f4c4d35432d0e17d212f2fae4e0f8247"></script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>