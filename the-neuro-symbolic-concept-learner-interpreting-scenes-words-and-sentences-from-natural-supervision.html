<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>The Neuro Symbolic Concept Learner: Interpreting scenes, words and sentences from natural supervision - Papers I read</title><meta name="description" content="The first passCategory: The paper tries to ground together multiple modalities. The hypothesis is that grounded learning helps in better generalisation without any explicit supervision.ContextJoint learning of vision and languagePeople have tried using attention modulesVideo captions/ Image Captions/ Visual Question AnsweringSome element of spatiovisual reasoning&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="./the-neuro-symbolic-concept-learner-interpreting-scenes-words-and-sentences-from-natural-supervision.html"><style>:root{--body-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--heading-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--logo-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--menu-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol"}</style><link rel="stylesheet" href="./assets/css/style.css?v=1d9126d5c592483966037cf187b30969"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"./the-neuro-symbolic-concept-learner-interpreting-scenes-words-and-sentences-from-natural-supervision.html"},"headline":"The Neuro Symbolic Concept Learner: Interpreting scenes, words and sentences from natural supervision","datePublished":"2020-11-11T13:48","dateModified":"2020-11-11T13:48","description":"The first passCategory: The paper tries to ground together multiple modalities. The hypothesis is that grounded learning helps in better generalisation without any explicit supervision.ContextJoint learning of vision and languagePeople have tried using attention modulesVideo captions/ Image Captions/ Visual Question AnsweringSome element of spatiovisual reasoning&hellip;","author":{"@type":"Person","name":"Samagra Sharma"},"publisher":{"@type":"Organization","name":"Samagra Sharma"}}</script><script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-XZP5EE6XT3"></script><script>window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'TRACKING_ID');</script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="./">Papers I read</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="./the-three-pass-approach.html" target="_self">The three pass approach</a></li></ul></nav></header><main><article class="post"><div class="hero"><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2020-11-11T13:48">November 11, 2020</time></div><h1>The Neuro Symbolic Concept Learner: Interpreting scenes, words and sentences from natural supervision</h1><div class="post__meta post__meta--author"><img src="./media/website/prof_pic.png" loading="eager" alt="Samagra Sharma" class="post__author-thumb"> <a href="./authors/samagra-sharma/" class="feed__author invert">Samagra Sharma</a></div></div></header></div><div class="wrapper post__entry"><h2>The first pass</h2><ul><li>Category: The paper tries to ground together multiple modalities. The hypothesis is that grounded learning helps in better generalisation without any explicit supervision.</li><li>Context<ul><li>Joint learning of vision and language</li><li>People have tried using attention modules</li><li>Video captions/ Image Captions/ Visual Question Answering</li><li>Some element of spatiovisual reasoning is also incorporated</li><li>Some element of interpretable/disentangled representations in terms of semantic parsing of natural language</li></ul></li><li>Correctness<ul><li>The idea of learning vision and language together seems natural. I am a bit unsure about the semantic parsing part and whether it could be called supervision or not but the general theme makes sense</li></ul></li><li>Contributions<ul><li>I dont exactly grasp the contributions in the first pass. Maybe this brings together things from multiple disciplines in order to ground the two modalities together.</li><li>Some form of curriculum learning</li><li>Some form of natural language parsing</li><li>Symbolic executor</li></ul></li><li>Clarity<ul><li>In an end to end manner I would have tried a VQA dataset with an attention mechanism for both the visual and linguistic component.</li><li>The supervision would be in the form of answers to the related questions</li><li>Input =&gt; (Image_features, Language embeddings of the question) initialised randomly but learnt in an end to end fashion =&gt; Output would be the answer to the question</li><li>I would have kept an initial few layers of the Vision and Language nets followed by a few concatenation layers (These concat layers would have been the grounding layers). The decoder would be the simplest model possible in order to include the reasoning capabilities into the concat layers.</li><li>Another approach would be a dual-conditioned attention mechanism. Visual attention conditioned on text embeddings and vice versa. Some form similar to that of a transformer -&gt; Maybe a key, hash, value vector combination for each patch in the image and a key, hash, value vector for each word. These two sets of vectors would be of similar dimensions in order to allow for conditioning of text with image patches and vice versa.</li><li>I don't have much experience with parsing based solutions and what exactly is meant by executable natural language programs. Even what does the term execution means in visuolingual setting. Am I changing the images? Am I changing the scenery?</li><li>What is meant by symbolic concept learning? Is it just a mapping? What exactly are the symbols mapped to? How are they interpretable?</li><li>Translating the question into an executable program? How? </li></ul></li></ul><h2>The second pass</h2><p> </p></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on November 11, 2020</p><div class="post__share"></div><div class="post__bio bio"><img class="bio__avatar" src="./media/website/prof_pic.png" loading="lazy" alt="Samagra Sharma"><div class="bio__info"><h3 class="bio__name"><a href="./authors/samagra-sharma/" class="invert" rel="author">Samagra Sharma</a></h3><p>Samagra is an AI Researcher at Adobe Media and Data Science Research Labs. He graduated from IIT Roorkee with a Bachelor&#x27;s in Computer Science and Engineering.</p></div></div></footer></article><nav class="post__nav"><div class="post__nav-inner"><div class="post__nav-prev"><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="./assets/svg/svg-map.svg#arrow-prev"/></svg> <a href="./winogrande-an-adversarial-winograd-schema-challenge-at-scale.html" class="invert post__nav-link" rel="prev"><span>Previous</span> WinoGrande: An Adversarial Winograd Schema Challenge at Scale</a></div></div></nav></main><footer class="footer"><div class="footer__social"><a href="https://www.facebook.com/samagra.sharma/" aria-label="Facebook"><svg><use xlink:href="./assets/svg/svg-map.svg#facebook"/></svg> </a><a href="https://twitter.com/samagra_sharma" aria-label="Twitter"><svg><use xlink:href="./assets/svg/svg-map.svg#twitter"/></svg> </a><a href="https://www.instagram.com/samagra14/" aria-label="Instagram"><svg><use xlink:href="./assets/svg/svg-map.svg#instagram"/></svg> </a><a href="https://www.linkedin.com/in/samagra-sharma-4476bb135/" aria-label="LinkedIn"><svg><use xlink:href="./assets/svg/svg-map.svg#linkedin"/></svg></a></div><div class="footer__copyright"><p>Powered by <a href="https://getpublii.com" target="_blank" rel="nofollow noopener">Publii</a></p></div><button class="footer__bttop js-footer__bttop" aria-label="Back to top"><svg><title>Back to top</title><use xlink:href="./assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'sidebar',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.top',
   };</script><script defer="defer" src="./assets/js/scripts.min.js?v=f4c4d35432d0e17d212f2fae4e0f8247"></script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>