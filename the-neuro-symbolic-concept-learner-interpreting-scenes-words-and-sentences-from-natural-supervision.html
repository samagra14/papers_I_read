<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>The Neuro Symbolic Concept Learner: Interpreting scenes, words and sentences from natural supervision - Papers I read</title><meta name="description" content="The first pass "><meta name="generator" content="Publii Open-Source CMS for Static Site"><link rel="canonical" href="./the-neuro-symbolic-concept-learner-interpreting-scenes-words-and-sentences-from-natural-supervision.html"><style>:root{--body-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--heading-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--logo-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--menu-font:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen,Ubuntu,Cantarell,"Fira Sans","Droid Sans","Helvetica Neue",Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol"}</style><link rel="stylesheet" href="./assets/css/style.css?v=1d9126d5c592483966037cf187b30969"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"./the-neuro-symbolic-concept-learner-interpreting-scenes-words-and-sentences-from-natural-supervision.html"},"headline":"The Neuro Symbolic Concept Learner: Interpreting scenes, words and sentences from natural supervision","datePublished":"2020-11-11T13:48","dateModified":"2020-11-17T20:21","image":{"@type":"ImageObject","url":"./media/posts/5/neural.jpeg","height":899,"width":1353},"description":"The first pass ","author":{"@type":"Person","name":"Samagra Sharma"},"publisher":{"@type":"Organization","name":"Samagra Sharma"}}</script><script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script>window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']]
  }
};</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-XZP5EE6XT3"></script><script>window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'TRACKING_ID');</script></head><body><div class="site-container"><header class="top" id="js-header"><a class="logo" href="./">Papers I read</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li><a href="./the-three-pass-approach.html" target="_self">The three pass approach</a></li></ul></nav></header><main><article class="post"><div class="hero"><figure class="hero__image hero__image--overlay"><img src="./media/posts/5/neural.jpeg" srcset="./media/posts/5/responsive/neural-xs.jpeg 300w, ./media/posts/5/responsive/neural-sm.jpeg 480w, ./media/posts/5/responsive/neural-md.jpeg 768w, ./media/posts/5/responsive/neural-lg.jpeg 1024w, ./media/posts/5/responsive/neural-xl.jpeg 1360w, ./media/posts/5/responsive/neural-2xl.jpeg 1600w" sizes="(max-width: 1600px) 100vw, 1600px" loading="eager" height="899" width="1353" alt="The Neuro Symbolic Concept Learner"><figcaption>Vision and language are bound to coexist, learning from the other and improving.</figcaption></figure><header class="hero__content"><div class="wrapper"><div class="post__meta"><time datetime="2020-11-11T13:48">November 11, 2020</time></div><h1>The Neuro Symbolic Concept Learner: Interpreting scenes, words and sentences from natural supervision</h1><div class="post__meta post__meta--author"><img src="./media/website/prof_pic.png" loading="eager" alt="Samagra Sharma" class="post__author-thumb"> <a href="./authors/samagra-sharma/" class="feed__author invert">Samagra Sharma</a></div></div></header></div><div class="wrapper post__entry"><p>The first pass</p><ul><li>Category: The paper tries to ground together multiple modalities. The hypothesis is that grounded learning helps in better generalisation without any explicit supervision.</li><li>Context<ul><li>Joint learning of vision and language</li><li>People have tried using attention modules</li><li>Video captions/ Image Captions/ Visual Question Answering</li><li>Some element of spatiovisual reasoning is also incorporated</li><li>Some element of interpretable/disentangled representations in terms of semantic parsing of natural language</li></ul></li><li>Correctness<ul><li>The idea of learning vision and language together seems natural. I am a bit unsure about the semantic parsing part and whether it could be called supervision or not but the general theme makes sense</li></ul></li><li>Contributions<ul><li>I dont exactly grasp the contributions in the first pass. Maybe this brings together things from multiple disciplines in order to ground the two modalities together.</li><li>Some form of curriculum learning</li><li>Some form of natural language parsing</li><li>Symbolic executor</li></ul></li><li>Clarity<ul><li>In an end to end manner I would have tried a VQA dataset with an attention mechanism for both the visual and linguistic component.</li><li>The supervision would be in the form of answers to the related questions</li><li>Input =&gt; (Image_features, Language embeddings of the question) initialised randomly but learnt in an end to end fashion =&gt; Output would be the answer to the question</li><li>I would have kept an initial few layers of the Vision and Language nets followed by a few concatenation layers (These concat layers would have been the grounding layers). The decoder would be the simplest model possible in order to include the reasoning capabilities into the concat layers.</li><li>Another approach would be a dual-conditioned attention mechanism. Visual attention conditioned on text embeddings and vice versa. Some form similar to that of a transformer -&gt; Maybe a key, hash, value vector combination for each patch in the image and a key, hash, value vector for each word. These two sets of vectors would be of similar dimensions in order to allow for conditioning of text with image patches and vice versa.</li><li>I don't have much experience with parsing based solutions and what exactly is meant by executable natural language programs. Even what does the term execution means in visuolingual setting. Am I changing the images? Am I changing the scenery?</li><li>What is meant by symbolic concept learning? Is it just a mapping? What exactly are the symbols mapped to? How are they interpretable?</li><li>Translating the question into an executable program? How? </li></ul></li></ul><h2>The second pass</h2><ul><li>Neuro symbolic Concept Learner<ul><li>Method <ul><li>First extract image features using a CNN and bounding boxes using RCNN to get object bounding boxes/region of interest.</li><li>A NN parser to parse the natural language sentence into a tree</li><li>Use that parse tree to apply neural net based transformations to the image feature vectors. </li><li>At the end of the transformations, use the nearest neighbour to return an answer</li></ul></li><li>The Domain Specific Language makes the entire algorithm interpretable and generalizable.</li><li>Deterministic</li><li>End to end differentiable hence, trainable<ul><li>Rl can train some forms of non differentiable functions</li></ul></li></ul></li><li>Model Details<ul><li>Visual Perception<ul><li>Pre trained mask RCNN to generate region based and image based features. </li><li>Both type of features are concatenated to provide context</li></ul></li><li>Concept Quantization<ul><li>Visual attributes are shape, size, color etx.<ul><li>Visual attributes are modeled as an embedding space</li><li>Each visual attribute has a seperate embedding space</li><li>A NN transforms the object representation to a vector in the visual attribute space</li></ul></li><li>Visual concepts are cube, sphere, sylinder, red, blue etc.<ul><li>Each concept is a vector in the attribute embedding space</li><li>Initialised randomly.</li><li>Learnt during the training</li><li>Some visual concepts are based on pairs of objects</li></ul></li><li>Cosine similarity is the criteria for identifying the attribute of a resulting vector. (For eg if an object vector is operated by a Color NN, and if the cosine similarity of the resulting vector is the highest with the Red Vector then the object's color is red)</li></ul></li><li>DSL and Semantic Parsing<ul><li>A domain specific language which operates on the attribute embedding space is designed.</li><li>A vocabulary of concepts is predecided.</li><li>Encode the sentence question to get a question embedding</li><li>Extract the concept words from the question.</li><li>Call the recursive parse function from the appendix<ul><li>This function contains additional neural operators which extract certain required items.</li></ul></li></ul></li><li>Quasi Symbolic Program Execution<ul><li>Given the program tree, each operation in the tree is performed on the visual feature vectors.</li><li>Each operator is a NN and outputs a differentiable vector </li><li>Trained using backprop</li></ul></li></ul></li><li>Training Paradigm<ul><li>Optimisation objective<ul><li>REINFORCE (Reinforcement Learning) is used to optimise the parser</li><li>Reward +1 if answer is correct else -1. </li></ul></li></ul></li><li>Experiments<ul><li>Classification based concepts evaluation- Tested on the validation split of CLEVR</li><li>Good Generalisation capabilities:<ul><li>New visual compositions (Unseen image and shape combinations) : 98.9 %</li><li>New visual concepts: 93.9 %</li><li>New scenes and questions:</li></ul></li></ul></li></ul><h2>Ideas</h2><ul><li>Automatic discovery of concept words.</li><li>Doing exactly this in a gaming simulation =&gt; Neural Parser, Ground language in Graphics</li><li> </li></ul><p> </p></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on November 17, 2020</p><div class="post__share"></div><div class="post__bio bio"><img class="bio__avatar" src="./media/website/prof_pic.png" loading="lazy" alt="Samagra Sharma"><div class="bio__info"><h3 class="bio__name"><a href="./authors/samagra-sharma/" class="invert" rel="author">Samagra Sharma</a></h3><p>Samagra is an AI Researcher at Adobe Media and Data Science Research Labs. He graduated from IIT Roorkee with a Bachelor&#x27;s in Computer Science and Engineering.</p></div></div></footer></article><nav class="post__nav"><div class="post__nav-inner"><div class="post__nav-prev"><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="./assets/svg/svg-map.svg#arrow-prev"/></svg> <a href="./winogrande-an-adversarial-winograd-schema-challenge-at-scale.html" class="invert post__nav-link" rel="prev"><span>Previous</span> WinoGrande: An Adversarial Winograd Schema Challenge at Scale</a></div><div class="post__nav-next"><a href="./reading-list.html" class="invert post__nav-link" rel="next"><span>Next</span> Reading List </a><svg width="1.041em" height="0.416em" aria-hidden="true"><use xlink:href="./assets/svg/svg-map.svg#arrow-next"/></svg></div></div></nav></main><footer class="footer"><div class="footer__social"><a href="https://www.facebook.com/samagra.sharma/" aria-label="Facebook"><svg><use xlink:href="./assets/svg/svg-map.svg#facebook"/></svg> </a><a href="https://twitter.com/samagra_sharma" aria-label="Twitter"><svg><use xlink:href="./assets/svg/svg-map.svg#twitter"/></svg> </a><a href="https://www.instagram.com/samagra14/" aria-label="Instagram"><svg><use xlink:href="./assets/svg/svg-map.svg#instagram"/></svg> </a><a href="https://www.linkedin.com/in/samagra-sharma-4476bb135/" aria-label="LinkedIn"><svg><use xlink:href="./assets/svg/svg-map.svg#linkedin"/></svg></a></div><div class="footer__copyright"><p>Powered by <a href="https://getpublii.com" target="_blank" rel="nofollow noopener">Publii</a></p></div><button class="footer__bttop js-footer__bttop" aria-label="Back to top"><svg><title>Back to top</title><use xlink:href="./assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = {    
        mobileMenuMode: 'sidebar',
        animationSpeed: 300,
        submenuWidth: 'auto',
        doubleClickTime: 500,
        mobileMenuExpandableSubmenus: true, 
        relatedContainerForOverlayMenuSelector: '.top',
   };</script><script defer="defer" src="./assets/js/scripts.min.js?v=f4c4d35432d0e17d212f2fae4e0f8247"></script><script>var images = document.querySelectorAll('img[loading]');

        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>